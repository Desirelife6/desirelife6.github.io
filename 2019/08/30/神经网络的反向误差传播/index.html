<!DOCTYPE html>












  


<html class="theme-next gemini use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.9.0">
<script src="https://cdn.jsdelivr.net/npm/jquery/dist/jquery.min.js"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome/css/font-awesome.min.css">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<script>
    (function () {
        if ('') {
            if (prompt('请输入文章密码') !== '') {
                alert('密码错误！');
                if (history.length === 1) {
                    location.replace("http://twifor.github.io"); // 这里替换成你的首页
                } else {
                    history.back();
                }
            }
        }
    })();
</script>




  
  
    
    
  <script src="/desirelife6.github.io/lib/pace/pace.min.js?v=1.0.2"></script>
  <link rel="stylesheet" href="/desirelife6.github.io/lib/pace/pace-theme-bounce.min.css?v=1.0.2">























<link rel="stylesheet" href="/desirelife6.github.io/lib/font-awesome/css/font-awesome.min.css?v=4.6.2">

<link rel="stylesheet" href="/desirelife6.github.io/css/main.css?v=6.7.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/desirelife6.github.io/images/twt.png?v=6.7.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/desirelife6.github.io/images/twt.png?v=6.7.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/desirelife6.github.io/images/twt.png?v=6.7.0">


  <link rel="mask-icon" href="/desirelife6.github.io/images/logo.svg?v=6.7.0" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/desirelife6.github.io/',
    scheme: 'Gemini',
    version: '6.7.0',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":true,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":true,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="一、一般的全连接神经网络结构1.1 “层”的概念每个“层”在神经网络中被实现为一个类，作为神经网络中的功能单位，类中定义了此层接受的输入，运算和输出。比如，负责激活函数的层就是Sigmoid层，ReLU层等等，负责矩阵相乘的就是上次提到的Affine层。以Affine层和ReLU层为例 123456789101112131415161718AffineX = np.random.rand(2)">
<meta name="keywords" content="neural network">
<meta property="og:type" content="article">
<meta property="og:title" content="神经网络的反向误差传播">
<meta property="og:url" content="https://github.com/Desirelife6/child/2019/08/30/神经网络的反向误差传播/index.html">
<meta property="og:site_name" content="Desirelife&#39;s blog">
<meta property="og:description" content="一、一般的全连接神经网络结构1.1 “层”的概念每个“层”在神经网络中被实现为一个类，作为神经网络中的功能单位，类中定义了此层接受的输入，运算和输出。比如，负责激活函数的层就是Sigmoid层，ReLU层等等，负责矩阵相乘的就是上次提到的Affine层。以Affine层和ReLU层为例 123456789101112131415161718AffineX = np.random.rand(2)">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/14338022-476c9832f7313def.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/14338022-bf9fe52bbc7d1934.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/14338022-9e3ec7e96920e915.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/14338022-bb50c362ac94ef56.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/14338022-9c0c549cb907e469.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/14338022-10e378e162715943.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/14338022-ebb7d7543a0b5ada.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/14338022-faaa0b9e9106880e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/14338022-8c39f66c66b71021.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/14338022-4a2ec924f87098a4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/14338022-3c46477a352904ce.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/14338022-ea38076a4e55f33b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/14338022-f4c7245e87e25ae5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/14338022-09ff8b433b17389e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/14338022-e6b98c30e408bbd3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/14338022-35231df4b6a9dfa3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/14338022-d0acf711f3e79f2d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/14338022-27e9bddf662daaf9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/14338022-b75afdcbcd27ce45.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/14338022-8cffa4844e0182bb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/14338022-2e7806efcaa77582.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/14338022-56a98d8ccd972ddc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/14338022-d9b6603ae324f3cb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/14338022-15543240f74047ec.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/14338022-7654f8a925e92202.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:updated_time" content="2019-08-31T01:30:17.273Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="神经网络的反向误差传播">
<meta name="twitter:description" content="一、一般的全连接神经网络结构1.1 “层”的概念每个“层”在神经网络中被实现为一个类，作为神经网络中的功能单位，类中定义了此层接受的输入，运算和输出。比如，负责激活函数的层就是Sigmoid层，ReLU层等等，负责矩阵相乘的就是上次提到的Affine层。以Affine层和ReLU层为例 123456789101112131415161718AffineX = np.random.rand(2)">
<meta name="twitter:image" content="https://upload-images.jianshu.io/upload_images/14338022-476c9832f7313def.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">






  <link rel="canonical" href="https://github.com/Desirelife6/child/2019/08/30/神经网络的反向误差传播/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>神经网络的反向误差传播 | Desirelife's blog</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">


  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>
	<a href="https://github.com/Desirelife6" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewbox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"/><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"/><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"/></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>
    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/desirelife6.github.io/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Desirelife's blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
      
        <p class="site-subtitle">一枚爱生活爱吃饭的可爱程序猿</p>
      
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
    
      
    

    

    <a href="/desirelife6.github.io/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    

    <a href="/desirelife6.github.io/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
    
      
    

    

    <a href="/desirelife6.github.io/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>分类</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
    
      
    

    

    <a href="/desirelife6.github.io/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-about">

    
    
    
      
    

    

    <a href="/desirelife6.github.io/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>关于</a>

  </li>

      
      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>搜索</a>
        </li>
      
    </ul>
  

  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



  



</div>
    </header>
    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://github.com/Desirelife6/child/desirelife6.github.io/2019/08/30/神经网络的反向误差传播/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Desirelife">
      <meta itemprop="description" content="一枚爱生活爱吃饭的可爱程序猿">
      <meta itemprop="image" content="/desirelife6.github.io/images/my_avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Desirelife's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">神经网络的反向误差传播

              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-08-30 22:11:00" itemprop="dateCreated datePublished" datetime="2019-08-30T22:11:00+08:00">2019-08-30</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-08-31 09:30:17" itemprop="dateModified" datetime="2019-08-31T09:30:17+08:00">2019-08-31</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/desirelife6.github.io/categories/neural-network/" itemprop="url" rel="index"><span itemprop="name">neural network</span></a></span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/desirelife6.github.io/2019/08/30/神经网络的反向误差传播/#comments" itemprop="discussionUrl">
                  <span class="post-meta-item-text">评论数：</span> <span class="post-comments-count valine-comment-count" data-xid="/desirelife6.github.io/2019/08/30/神经网络的反向误差传播/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="一、一般的全连接神经网络结构"><a href="#一、一般的全连接神经网络结构" class="headerlink" title="一、一般的全连接神经网络结构"></a>一、一般的全连接神经网络结构</h2><h3 id="1-1-“层”的概念"><a href="#1-1-“层”的概念" class="headerlink" title="1.1 “层”的概念"></a>1.1 “层”的概念</h3><p>每个“层”在神经网络中被实现为一个类，作为神经网络中的功能单位，类中定义了此层接受的输入，运算和输出。比如，负责激活函数的层就是Sigmoid层，ReLU层等等，负责矩阵相乘的就是上次提到的Affine层。<br>以Affine层和ReLU层为例</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">Affine</span><br><span class="line">X = np.random.rand(<span class="number">2</span>)  <span class="comment"># 输入</span></span><br><span class="line">W = np.random.rand(<span class="number">2</span>，<span class="number">3</span>)  <span class="comment"># 权重参数</span></span><br><span class="line">B = np.random.rand(<span class="number">3</span>)  <span class="comment"># 偏置</span></span><br><span class="line">Y = np.dot(X, W) + B    <span class="comment"># 得到相乘结果</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Relu</span>:</span>    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span>        </span><br><span class="line">        self.mask = <span class="literal">None</span>        <span class="comment"># mask这个变量在反向传播时很重要，稍后会提到</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span>        <span class="comment"># 正向传播</span></span><br><span class="line">        self.mask = (x &lt;= <span class="number">0</span>)        </span><br><span class="line">        out = x.copy()        </span><br><span class="line">        out[self.mask] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self, dout)</span>:</span>        <span class="comment"># 反向传播</span></span><br><span class="line">        dout[self.mask] = <span class="number">0</span>        </span><br><span class="line">        dx = dout</span><br><span class="line">        <span class="keyword">return</span> dx</span><br></pre></td></tr></table></figure>

<p><img src="https://upload-images.jianshu.io/upload_images/14338022-476c9832f7313def.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="1-1 神经网络示意图"></p>
<p><img src="https://upload-images.jianshu.io/upload_images/14338022-bf9fe52bbc7d1934.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="1-2 简单神经网络的结构"><br>这里Affine中的 X、W、B 分别是形状为 (2,)、(2,3)、(3,) 的多维数组。这样一来，神经元的加权和可以用 Y = np.dot(X, W) + B 计算出来。然后，Y 经过激活函数转换后，传递给下一层。这就是神经网络正向传播的流程。调用激活层中的 forward 正向传播。</p>
<h2 id="二、正向传播时的梯度下降法"><a href="#二、正向传播时的梯度下降法" class="headerlink" title="二、正向传播时的梯度下降法"></a>二、正向传播时的梯度下降法</h2><p>在正向传播中，采用数值微分法求梯度<br>在每一层，采用数值微分法分别求损失函数对于该层的各个权重参数的偏导数，组合成梯度，然后令各参数向梯度下降的方向更新。代码其实很好懂，简直就像大白话，这里举一个简单的例子。<br> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 求梯度</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">numerical_gradient</span><span class="params">(f, x)</span>:</span>    <span class="comment"># x为传入的各参数的数组/多维数组</span></span><br><span class="line">    h = <span class="number">1e-4</span> <span class="comment"># 0.0001    </span></span><br><span class="line">    grad = np.zeros_like(x) <span class="comment"># 生成和x形状相同的数组</span></span><br><span class="line">    <span class="keyword">for</span> idx <span class="keyword">in</span> range(x.size):        </span><br><span class="line">        tmp_val = x[idx]    </span><br><span class="line">         <span class="comment"># f(x+h)的计算           </span></span><br><span class="line">        x[idx] = tmp_val + h        </span><br><span class="line">        fxh1 = f(x)</span><br><span class="line">        <span class="comment"># f(x-h)的计算        </span></span><br><span class="line">        x[idx] = tmp_val - h        </span><br><span class="line">        fxh2 = f(x)</span><br><span class="line">        <span class="comment"># 组合为梯度</span></span><br><span class="line">        grad[idx] = (fxh1 - fxh2) / (<span class="number">2</span>*h)        </span><br><span class="line">        x[idx] = tmp_val <span class="comment"># 还原值</span></span><br><span class="line">    <span class="keyword">return</span> grad</span><br><span class="line"></span><br><span class="line"><span class="comment"># 梯度下降法</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_descent</span><span class="params">(f, init_x, lr=<span class="number">0.01</span>, step_num=<span class="number">100</span>)</span>:</span>  </span><br><span class="line">    x = init_x</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(step_num):       </span><br><span class="line">        grad = numerical_gradient(f, x)        </span><br><span class="line">        x -= lr * grad</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure></p>
<p>梯度下降法的函数的参数中，f 为我们选用的损失函数，x为各层传入的参数数组，lr为学习率，也就是参数每次更新的程度，step_num为更新的轮数。</p>
<p><strong>这就是通过数值微分法计算损失函数关于权重参数的梯度。数值微分虽然简单，也容易实现，但缺点是计算上比较费时间。<br>所以我们将学习一个能够高效计算权重参数的梯度的方法——误差反向传播法。</strong> </p>
<h2 id="三、误差反向传播法"><a href="#三、误差反向传播法" class="headerlink" title="三、误差反向传播法"></a>三、误差反向传播法</h2><h3 id="3-1-计算图"><a href="#3-1-计算图" class="headerlink" title="3.1 计算图"></a>3.1 计算图</h3><h4 id="3-1-1-利用计算图求解"><a href="#3-1-1-利用计算图求解" class="headerlink" title="3.1.1 利用计算图求解"></a>3.1.1 利用计算图求解</h4><p>计算图解题的情况下，需要按如下流程进行</p>
<ol>
<li>构建计算图。 </li>
<li>在计算图上，从左向右进行计算。（正向传播）</li>
</ol>
<p>现在我们尝试用计算图求解简单的问题<br>问题1： 太郎在超市买了2个100元一个的苹果，消费税是10%，请计算支付金额。<br><img src="https://upload-images.jianshu.io/upload_images/14338022-9e3ec7e96920e915.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="3-1 计算图求解问题1"></p>
<p>问题2：太郎在超市买了2个苹果、3个橘子。其中，苹果每个100元， 橘子每个150元。消费税是10%，请计算支付金额。<br><img src="https://upload-images.jianshu.io/upload_images/14338022-bb50c362ac94ef56.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="3-2 计算图求解问题2"></p>
<h4 id="3-1-2-局部计算"><a href="#3-1-2-局部计算" class="headerlink" title="3.1.2 局部计算"></a>3.1.2 局部计算</h4><p>计算图的特征是可以通过传递“局部计算”获得最终结果。“局部”这个 词的意思是“与自己相关的某个小范围”。局部计算是指，无论全局发生了什么， 都能只根据与自己相关的信息输出接下来的结果。<br>我们用一个具体的例子来说明局部计算。比如，在超市买了2个苹果和其他很多东西。此时，可以画出如图3-3所示的计算图。<br><img src="https://upload-images.jianshu.io/upload_images/14338022-9c0c549cb907e469.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="3-3 局部计算"></p>
<p>如图3-3所示，假设（经过复杂的计算）购买的其他很多东西总共花费4000元。这里的重点是，各个节点处的计算都是局部计算。这意味着，例如苹果和其他很多东西的求和运算（4000 + 200 → 4200）并不关心4000这个数字是如何计算而来的，只要把两个数字相加就可以了。换言之，各个节点处只需进行与自己有关的计算（在这个例子中是对输入的两个数字进行加法运算），不用考虑全局。<br>综上，计算图可以集中精力于局部计算。无论全局的计算有多么复杂， 各个步骤所要做的就是对象节点的局部计算。虽然局部计算非常简单，但是通过传递它的计算结果，可以获得全局的复杂计算的结果。</p>
<h4 id="3-1-3-为何使用计算图"><a href="#3-1-3-为何使用计算图" class="headerlink" title="3.1.3 为何使用计算图"></a>3.1.3 为何使用计算图</h4><p>实际上，使用计算图最大的原因是，可以通过反向传播高效计算导数。 在介绍计算图的反向传播时，我们再来思考一下问题1。问题1中，假设我们想知道苹果价格的上涨会在多大程度上影响最终的支付金额，即求“支付金额关于苹果的价格的导数”。设苹果的价格为 x，支付金额为 L，则相当于求 $ \frac{dL}{dX} $。<br>先来看一下结果，如图3-4所示，可以通过计算图的反向传播求导数（关于如何进行反向传播，接下来马上会介绍）。<br><img src="https://upload-images.jianshu.io/upload_images/14338022-10e378e162715943.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="3-2 反向传播求导数"><br>如图3-4所示，反向传播使用与正方向相反的箭头（粗线）表示。反向传播传递“局部导数”，将导数的值写在箭头的下方。在这个例子中，反向传播从右向左传递导数的值（1→1.1→2.2）。从这个结果中可知，“支付金额关于苹果的价格的导数”的值是2.2</p>
<p>“支付金额关于消费税的导数”“支付金额关于苹果的个数的导数”等也都可以用同样的方式算出来。并且, 计算中途求得的导数的结果（中间传递的导数）可以被共享，从而可以高效地计算多个导数。综上，计算图的优点是，可以通过正向传播和反向传 播高效地计算各个变量的导数值。</p>
<h3 id="3-2-链式法则"><a href="#3-2-链式法则" class="headerlink" title="3.2  链式法则"></a>3.2  链式法则</h3><h4 id="3-2-1-计算图的反向传播"><a href="#3-2-1-计算图的反向传播" class="headerlink" title="3.2.1 计算图的反向传播"></a>3.2.1 计算图的反向传播</h4><p>话不多说，让我们先来看一个使用计算图的反向传播的例子。假设存在 y = f(x)的计算，这个计算的反向传播如图3-4所示。<br><img src="https://upload-images.jianshu.io/upload_images/14338022-ebb7d7543a0b5ada.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="3-4 计算图的反向传播"></p>
<p>如图所示，反向传播的计算顺序是，将信号E乘以节点的局部导数 （ $\frac{dy}{dx}$），然后将结果传递给下一个节点。这里所说的局部导数是指正向传播中$y = f(x) $的导数，也就是y关于x的导数（ $\frac{dy}{dx}$）。把这个局部导数乘以上游传过来的值（本例中为E）， 然后传递给前面的节点。</p>
<h4 id="3-2-2-链式法则"><a href="#3-2-2-链式法则" class="headerlink" title="3.2.2 链式法则"></a>3.2.2 链式法则</h4><p>链式法则是关于复合函数的导数的性质，定义如下。</p>
<blockquote>
<p>如果某个函数由复合函数表示，则该复合函数的导数可以用构成复合函数的各个函数的导数的乘积表示。</p>
</blockquote>
<p>形如 $\frac{dZ}{dX}  =  \frac{dZ}{dt}  \frac{dt}{dZ} $， 即为求导的链式法则。高数内容，不再赘述。</p>
<h4 id="3-2-3-链式法则与计算图"><a href="#3-2-3-链式法则与计算图" class="headerlink" title="3.2.3 链式法则与计算图"></a>3.2.3 链式法则与计算图</h4><p>现在我们尝试将上式的链式法则的计算用计算图表示出来。如果用 “**2”节点表示平方运算的话，则计算图如图3-5所示。<br><img src="https://upload-images.jianshu.io/upload_images/14338022-faaa0b9e9106880e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="3-5 链式法则的计算图表示"><br>因为$\frac{dz}{dt} = 2t $; $\frac{dt}{dx} = 1 $ ; 所以$\frac{dz}{dx}  = 2(x+y) $</p>
<h3 id="3-3-反向传播"><a href="#3-3-反向传播" class="headerlink" title="3.3 反向传播"></a>3.3 反向传播</h3><h4 id="3-3-1-加法的反向传播"><a href="#3-3-1-加法的反向传播" class="headerlink" title="3.3.1 加法的反向传播"></a>3.3.1 加法的反向传播</h4><p>加法反向传播将从上游传过来的导数（本例中是 ）乘以 1，然 后传向下游。也就是说，因为加法节点的反向传播只乘以1，所以输入的值 会原封不动地流向下一个节点。<br><img src="https://upload-images.jianshu.io/upload_images/14338022-8c39f66c66b71021.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="3-6 加法的反向传播"><br><img src="https://upload-images.jianshu.io/upload_images/14338022-4a2ec924f87098a4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="3-7 加法的具体例子"></p>
<h4 id="3-3-2-乘法的反向传播"><a href="#3-3-2-乘法的反向传播" class="headerlink" title="3.3.2 乘法的反向传播"></a>3.3.2 乘法的反向传播</h4><p>乘法的反向传播会将上游的值乘以正向传播时的输入信号的“翻转值” 后传递给下游。翻转值表示一种翻转关系，如图5-12所示，正向传播时信号 是x的话，反向传播时则是y；正向传播时信号是y的话，反向传播时则是x。<br><img src="https://upload-images.jianshu.io/upload_images/14338022-3c46477a352904ce.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="3-8 乘法反向传播"></p>
<p> <img src="https://upload-images.jianshu.io/upload_images/14338022-ea38076a4e55f33b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="3-9 乘法的具体例子"></p>
<h4 id="练习"><a href="#练习" class="headerlink" title="练习"></a>练习</h4><p>结合加法和乘法，以苹果为例，尝试填入这些空格</p>
<p><img src="https://upload-images.jianshu.io/upload_images/14338022-f4c7245e87e25ae5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="3-10 苹果问题的反向传播练习"></p>
<p><img src="https://upload-images.jianshu.io/upload_images/14338022-09ff8b433b17389e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="3-11 苹果问题答案"></p>
<h3 id="3-4-激活函数层的反向传播实现"><a href="#3-4-激活函数层的反向传播实现" class="headerlink" title="3.4 激活函数层的反向传播实现"></a>3.4 激活函数层的反向传播实现</h3><h4 id="3-4-1-ReLU-层"><a href="#3-4-1-ReLU-层" class="headerlink" title="3.4.1 ReLU 层"></a>3.4.1 ReLU 层</h4><p>激活函数ReLU的导数为<img src="https://upload-images.jianshu.io/upload_images/14338022-e6b98c30e408bbd3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt><br>如果正向传播时的输入x大于0，则反向传播会将上游的 值原封不动地传给下游。反过来，如果正向传播时的x小于等于0，则反向传播中传给下游的信号将停在此处<br>所以在开始时提到的ReLU层的mask变量，是用来存储正向传播输入是否为0的变量，它是由True/False构成的NumPy数组，它会把正向传播时的输入x的元素中小于等于0的地方保存为True，其他地方（大于0的元素）保存为False。在反向传播中尤其重要。<br>计算图表示即为<img src="https://upload-images.jianshu.io/upload_images/14338022-35231df4b6a9dfa3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p>
<h4 id="3-4-2-Sigmoid层"><a href="#3-4-2-Sigmoid层" class="headerlink" title="3.4.2 Sigmoid层"></a>3.4.2 Sigmoid层</h4><p>步骤1<br>“/”节点表示 ，它的导数可以解析性地表示为下式。<br>$ \frac{dy}{dx} = - \frac{1}{x^2} = - y^2 $<br>反向传播时，会将上游的值乘以$−y^2$（正向传播的输出的平方乘以−1后的值）后，再传给下游。</p>
<p>步骤2<br> “+”节点将上游的值原封不动地传给下游。</p>
<p>步骤3<br>“exp”节点表示y = exp(x)，它的导数由下式表示。<br>$ \frac{dy}{dx} = exp(x) $<br>计算图中，上游的值乘以正向传播时的输出（这个例子中是exp(−x)）后， 再传给下游。</p>
<p>步骤4<br>“×”节点将正向传播时的值翻转后做乘法运算。因此，这里要乘以−1。</p>
<p>最终结果为</p>
<p><img src="https://upload-images.jianshu.io/upload_images/14338022-d0acf711f3e79f2d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="3-12 Sigmoid层的计算图"><br>这里要注意， 这个值只根据正向传播时的输入x和输出y就可以算出来,所以sigmoid的计算图可以简化为sigmoid节点，简洁版的计算图可以省略反向传播中的计算过程，因此计算效率更高。此外， 通过对节点进行集约化，可以不用在意Sigmoid层中琐碎的细节，而只需要专注它的输入和输出，这一点也很重要。<br><img src="https://upload-images.jianshu.io/upload_images/14338022-27e9bddf662daaf9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="3-13 Sigmoid层的简洁版计算图"></p>
<h3 id="3-5-Affine-Softmax层的实现"><a href="#3-5-Affine-Softmax层的实现" class="headerlink" title="3.5 Affine/Softmax层的实现"></a>3.5 Affine/Softmax层的实现</h3><h4 id="3-5-1-Affine层"><a href="#3-5-1-Affine层" class="headerlink" title="3.5.1 Affine层"></a>3.5.1 Affine层</h4><p>神经网络的正向传播中，为了计算加权信号的总和，使用了矩阵的乘积运算</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Affine</span><br><span class="line">X = np.random.rand(<span class="number">2</span>)  <span class="comment"># 输入</span></span><br><span class="line">W = np.random.rand(<span class="number">2</span>，<span class="number">3</span>)  <span class="comment"># 权重参数</span></span><br><span class="line">B = np.random.rand(<span class="number">3</span>)  <span class="comment"># 偏置</span></span><br><span class="line">Y = np.dot(X, W) + B    <span class="comment"># 得到相乘结果</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>神经网络的正向传播中进行的矩阵的乘积运算在几何学领域被称为“仿 射变换” A。因此，这里将进行仿射变换的处理实现为“Affine层”。</p>
</blockquote>
<p>现在将它用计算图表示出来，要注意X、W、B是矩阵（多维数组）。 之前我们见到的计算图中各个节点间流动的是标量，而这个例子中各个节点间传播的是矩阵。<br><img src="https://upload-images.jianshu.io/upload_images/14338022-b75afdcbcd27ce45.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="3-14 Affine层计算图"><br>现在我们来考虑图 3-14 的计算图的反向传播。以矩阵为对象的反向传播，通过数学推导可得到下式<br>$$ \frac{dL}{dX} = \frac{dL}{dY} * W^T $$<br>$$ \frac{dL}{dW} = X^T * \frac{dL}{dY} $$<br>表示为计算图即为：<br><img src="https://upload-images.jianshu.io/upload_images/14338022-8cffa4844e0182bb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="3-15Affine的反向传播"><br>其中值得注意的是,X与$\frac{dL}{dX}$形状相同，W与$\frac{dL}{dW}$形状相同,从下式也容易看出</p>
<p><img src="https://upload-images.jianshu.io/upload_images/14338022-2e7806efcaa77582.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p>
<h4 id="3-5-2-批版本的Affine层"><a href="#3-5-2-批版本的Affine层" class="headerlink" title="3.5.2 批版本的Affine层"></a>3.5.2 批版本的Affine层</h4><p>前面介绍的Affi  ne层的输入X是以单个数据为对象的。现在我们考虑N 个数据一起进行正向传播的情况，也就是批版本的Affine层。<br>先来看计算图<br><img src="https://upload-images.jianshu.io/upload_images/14338022-56a98d8ccd972ddc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="3-16 批版本Affine层"><br>与刚刚不同的是，现在输入X的形状是(N,2)。之后就和前面一样，在计算图上进行单纯的矩阵计算。<br><strong>加上偏置时，需要特别注意。正向传播时，偏置被加到X·W的各个数据上。比如，N = 2（数据为2个）时，偏置会被分别加到这2个数据（各自 的计算结果）上。因此， 反向传播时，各个数据的反向传播的值需要汇总为偏置的元素。</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>dY = np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>,], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]]) </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>dY </span><br><span class="line">array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],       </span><br><span class="line">       [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]]) </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>dB = np.sum(dY, axis=<span class="number">0</span>) </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>dB </span><br><span class="line">array([<span class="number">5</span>, <span class="number">7</span>, <span class="number">9</span>])</span><br></pre></td></tr></table></figure>

<p>这个例子中，假定数据有2个（N = 2）。偏置的反向传播会对这2个数据的导数按元素进行求和。因此，这里使用了np.sum()对第0轴（以数据为单位的轴，axis=0）方向上的元素进行求和。 </p>
<h4 id="3-5-3-Softmax-with-Loss-层"><a href="#3-5-3-Softmax-with-Loss-层" class="headerlink" title="3.5.3 Softmax-with-Loss 层"></a>3.5.3 Softmax-with-Loss 层</h4><p>上次我们提到过，softmax函数会将输入值正规化之后再输出。比如手写数字识别时，Softmax层的输出如下所示。<br><img src="https://upload-images.jianshu.io/upload_images/14338022-d9b6603ae324f3cb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=" 输入图像通过Aﬃne层和ReLU层进行转换，10个输入通过Softmax层进行正规化。在这个例子中，“0”的得分是5.3，这个值经过Softmax层转换为0.008 （0.8%）；“ 2”的得分是10.1，被转换为0.991（99.1%）
"></p>
<blockquote>
<p>神经网络中进行的处理有推理（inference）和学习两个阶段。神经网络的推理通常不使用Softmax层。比如，用上图的网络进行推理时， 会将最后一个Affine层的输出作为识别结果。神经网络中未被正规化的输出结果（Softmax层前面的Affine层的输出）有时被称为“得分”。也就是说，当神经网络的推理只需要给出一个答案的情况下，因为此时只对得分最大值感兴趣，所以不需要Softmax层。 不过，神经网络的学习阶段则需要Softmax层。</p>
</blockquote>
<p>来看softmax的计算图，导出过程比较复杂，这里只给出最终结果<br><img src="https://upload-images.jianshu.io/upload_images/14338022-15543240f74047ec.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="softmax计算图"></p>
<p>不感兴趣的话可以看简化版本<br><img src="https://upload-images.jianshu.io/upload_images/14338022-7654f8a925e92202.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="3-17 简化版softmax计算图"></p>
<p>Softmax层的反向传播得到了 $（y_1 −t_1,y_2 −t_2,y_3 −t_3）$这样“漂亮”的结果。由于$（y_1,y_2,y_3）$是 Softmax层的输出，$（ t_1,t_2,t_3）$是监督数据，所以$（y_1 −t_1,y_2 −t_2,y_3 −t_3）$是 Softmax层的输出和教师标签的差分。神经网络的反向传播会把这个差分表示的误差传递给前面的层，这是神经网络学习中的重要性质。<br>神经网络学习的目的就是通过调整权重参数，使神经网络的输出（Softmax 的输出）接近教师标签。因此，必须将神经网络的输出与教师标签的误差高效地传递给前面的层。刚刚的$（y_1 −t_1,y_2 −t_2,y_3 −t_3）$正是 Softmax层的输出与教师标签的差，直截了当地表示了当前神经网络的输出与教师标签的误差。 </p>
<blockquote>
<p>使用交叉熵误差作为softmax函数的损失函数后，反向传播得到 $（y_1 −t_1,y_2 −t_2,y_3 −t_3）$这样 “漂亮”的结果。实际上，这样“漂亮” 的结果并不是偶然的，而是为了得到这样的结果，特意设计了交叉熵误差函数。回归问题中输出层使用“恒等函数”，损失函数使用 “平方和误差”，也是出于同样的理由。也就是说，使用“平 方和误差”作为“恒等函数”的损失函数，反向传播才能得到$（y_1 −t_1,y_2 −t_2,y_3 −t_3）$这样“漂亮”的结果。</p>
</blockquote>
<h2 id="四、神经网络学习全貌"><a href="#四、神经网络学习全貌" class="headerlink" title="四、神经网络学习全貌"></a>四、神经网络学习全貌</h2><p>前提<br>神经网络中有合适的权重和偏置，调整权重和偏置以便拟合训练数据的过程称为学习<br>神经网络的学习分为下面4个步骤。<br>步骤1（mini-batch）<br>从训练数据中随机选择一部分数据。<br>步骤2（计算梯度）<br>计算损失函数关于各个权重参数的梯度。<br>步骤3（更新参数）<br>将权重参数沿梯度方向进行微小的更新。<br>步骤4（重复）<br>重复步骤1、步骤2、步骤3。<br>这次介绍的误差反向传播法会在步骤2中出现。上一次中，我们利用数值微分求得了这个梯度。数值微分虽然实现简单，但是计算要耗费较多的时间，而误差反向传播法可以快速高效地计算梯度。</p>
<p><strong>还有一篇CNN噢，欢迎持续关注 :-D</strong></p>

      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/desirelife6.github.io/tags/neural-network/" rel="tag"><i class="fa fa-tag"></i>neural network</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/desirelife6.github.io/2019/08/30/hello-world/" rel="next" title="Hello World">
                <i class="fa fa-chevron-left"></i> Hello World
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/desirelife6.github.io/2019/09/02/初识卷积神经网络（CNN）/" rel="prev" title="初始卷积神经网络（CNN）">
                初始卷积神经网络（CNN） <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  
    <div class="comments" id="comments">
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/desirelife6.github.io/images/my_avatar.jpg" alt="Desirelife">
            
              <p class="site-author-name" itemprop="name">Desirelife</p>
              <p class="site-description motion-element" itemprop="description">一枚爱生活爱吃饭的可爱程序猿</p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/desirelife6.github.io/archives/">
                
                    <span class="site-state-item-count">10</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  <a href="/desirelife6.github.io/categories/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">3</span>
                    <span class="site-state-item-name">分类</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/desirelife6.github.io/tags/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">5</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://github.com/Desirelife6" title="GitHub &rarr; https://github.com/Desirelife6"><i class="fa fa-fw fa-github"></i>GitHub</a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="mailto:897644126@qq.com" title="E-Mail &rarr; mailto:897644126@qq.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                </span>
              
            </div>
          

          

          
          

          
            
          
          

        </div>
      </div>

      
      <!--noindex-->
        <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#一、一般的全连接神经网络结构"><span class="nav-number">1.</span> <span class="nav-text">一、一般的全连接神经网络结构</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-“层”的概念"><span class="nav-number">1.1.</span> <span class="nav-text">1.1 “层”的概念</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#二、正向传播时的梯度下降法"><span class="nav-number">2.</span> <span class="nav-text">二、正向传播时的梯度下降法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#三、误差反向传播法"><span class="nav-number">3.</span> <span class="nav-text">三、误差反向传播法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-计算图"><span class="nav-number">3.1.</span> <span class="nav-text">3.1 计算图</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-1-利用计算图求解"><span class="nav-number">3.1.1.</span> <span class="nav-text">3.1.1 利用计算图求解</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-2-局部计算"><span class="nav-number">3.1.2.</span> <span class="nav-text">3.1.2 局部计算</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-3-为何使用计算图"><span class="nav-number">3.1.3.</span> <span class="nav-text">3.1.3 为何使用计算图</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-链式法则"><span class="nav-number">3.2.</span> <span class="nav-text">3.2  链式法则</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-1-计算图的反向传播"><span class="nav-number">3.2.1.</span> <span class="nav-text">3.2.1 计算图的反向传播</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-2-链式法则"><span class="nav-number">3.2.2.</span> <span class="nav-text">3.2.2 链式法则</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-3-链式法则与计算图"><span class="nav-number">3.2.3.</span> <span class="nav-text">3.2.3 链式法则与计算图</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-反向传播"><span class="nav-number">3.3.</span> <span class="nav-text">3.3 反向传播</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-1-加法的反向传播"><span class="nav-number">3.3.1.</span> <span class="nav-text">3.3.1 加法的反向传播</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-2-乘法的反向传播"><span class="nav-number">3.3.2.</span> <span class="nav-text">3.3.2 乘法的反向传播</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#练习"><span class="nav-number">3.3.3.</span> <span class="nav-text">练习</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-4-激活函数层的反向传播实现"><span class="nav-number">3.4.</span> <span class="nav-text">3.4 激活函数层的反向传播实现</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-4-1-ReLU-层"><span class="nav-number">3.4.1.</span> <span class="nav-text">3.4.1 ReLU 层</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-4-2-Sigmoid层"><span class="nav-number">3.4.2.</span> <span class="nav-text">3.4.2 Sigmoid层</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-5-Affine-Softmax层的实现"><span class="nav-number">3.5.</span> <span class="nav-text">3.5 Affine/Softmax层的实现</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-5-1-Affine层"><span class="nav-number">3.5.1.</span> <span class="nav-text">3.5.1 Affine层</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-5-2-批版本的Affine层"><span class="nav-number">3.5.2.</span> <span class="nav-text">3.5.2 批版本的Affine层</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-5-3-Softmax-with-Loss-层"><span class="nav-number">3.5.3.</span> <span class="nav-text">3.5.3 Softmax-with-Loss 层</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#四、神经网络学习全貌"><span class="nav-number">4.</span> <span class="nav-text">四、神经网络学习全貌</span></a></li></ol></div>
            

          </div>
        </div>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-heartbeat"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Desirelife6</span>

  

  
</div>









        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
          <span id="scrollpercent"><span>0</span>%</span>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>














  
    
    
  
  <script color="0,0,255" ' opacity="0.85" zindex="-1" count="99" src="/desirelife6.github.io/lib/canvas-nest/canvas-nest-nomobile.min.js"></script>













  
  <script src="/desirelife6.github.io/lib/jquery/index.js?v=2.1.3"></script>

  
  <script src="/desirelife6.github.io/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/desirelife6.github.io/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>


  


  <script src="/desirelife6.github.io/js/src/utils.js?v=6.7.0"></script>

  <script src="/desirelife6.github.io/js/src/motion.js?v=6.7.0"></script>



  
  


  <script src="/desirelife6.github.io/js/src/affix.js?v=6.7.0"></script>

  <script src="/desirelife6.github.io/js/src/schemes/pisces.js?v=6.7.0"></script>




  
  <script src="/desirelife6.github.io/js/src/scrollspy.js?v=6.7.0"></script>
<script src="/desirelife6.github.io/js/src/post-details.js?v=6.7.0"></script>



  


  <script src="/desirelife6.github.io/js/src/bootstrap.js?v=6.7.0"></script>



  
  




  

<script src="//cdn1.lncld.net/static/js/3.11.1/av-min.js"></script>



<script src="//unpkg.com/valine/dist/Valine.min.js"></script>

<script>
  var GUEST = ['nick', 'mail', 'link'];
  var guest = 'nick,mail,link';
  guest = guest.split(',').filter(function (item) {
    return GUEST.indexOf(item) > -1;
  });
  new Valine({
    el: '#comments',
    verify: false,
    notify: false,
    appId: 'n9UuiH8iAooE6proByRtdAX8-gzGzoHsz',
    appKey: 'xvkM0lzjv3J3PhBYcN447hWI',
    placeholder: 'Leave a message...',
    avatar: 'mm',
    meta: guest,
    pageSize: '10' || 10,
    visitor: false
  });
</script>



  

  <script>
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/desirelife6.github.io/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url).replace(/\/{2,}/g, '/');
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x"></i></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x"></i></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  

  
  

  
  

  


  

  

  

  

  

  

  

  

<script src="/js/src/snow.js"></script>
<style type="text/css">
.snow-container{position:fixed;top:0;left:0;width:100%;height:100%;pointer-events:none;z-index:100001;}</style>
<div class="snow-container"></div>
</body>
</html>
