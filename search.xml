<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[初始卷积神经网络（CNN）]]></title>
    <url>%2Fdesirelife6.github.io%2F2019%2F09%2F02%2F%E5%88%9D%E8%AF%86%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89%2F</url>
    <content type="text"><![CDATA[一、基础结构CNN和之 前介绍的神经网络一样，可以像乐高积木一样通过组装层来构建。不过， CNN中新出现了卷积层（Convolution层）和池化层（Pooling层）。此外，各层中传递的数据是有形状的数据（比如，3维数据）靠近输出的层中使用了之前 的“ Affine - ReLU”组合。此外，最后的输出层中使用了之前的“Affine - Softmax”组合。这些都是一般的CNN中比较常见的结构。 二、卷积层2.1 卷积层的优势在全连接层中，数据的形状被忽视了。以图像输入为例子，图像通常是高、长、通道方向上的3维形状。但是，向全连接层输入时，需要将3维数据拉平为1维数据。前面提到的使用 了MNIST数据集的例子中，输入图像就是1通道、高28像素、长28像素的（1, 28, 28）形状，但却被排成1列，以784个数据的形式输入到最开始的 Affine层。而卷积层可以保持形状不变。当输入数据是图像时，卷积层会以3维数据的形式接收输入数据，并同样以3维数据的形式输出至下一层。因此可以提取出邻近的像素为相似的值、RBG的各个通道之间分别有密切的关联性、相距 较远的像素之间没有什么关联等值得提取的本质模式，利用与数据形状相关的信息。 2.2 卷积运算（图像处理中的滤波器运算）2.2.1 卷积运算基本过程先来看一个经过卷积运算的结果对于输入数据，卷积运算以一定间隔滑动滤波器的窗口并应用。这里所说的窗口是指下图中灰色的3×3的部分。如下图所示，将各个位置上滤波器的元素和输入的对应元素相乘，然后再求和（有时将这个计算称为乘积累加运算）。然后，将这个结果保存到输出的对应位置。将这个过程在所有位置都进行一遍，就可以得到卷积运算的输出。 如果有偏置，则在卷积运算结果的各个位置都加上偏置的值。 2.2.2 填充在卷积神经网络中引入了填充和步幅等特殊概念，这里我们来介绍一下填充。在进行卷积层的处理之前，有时要向输入数据的周围填入固定的数据（比如0等），这称为填充（padding），是卷积运算中经常会用到的处理。比如， 在下图的例子中，对大小为(4, 4)的输入数据应用了幅度为1的填充。“幅度为1的填充”是指用幅度为1像素的0填充周围。可以看到，通过填充，大小为(4, 4)的输入数据变成了(6,6)的形状。 然后，应用大小为(3, 3)的滤波器，生成了大小为(4,4)的输出数据。这个例子中将填充的值设成了1，不过填充的值也可以设置成2、3等任意的整数。在图该例子中，如果将填充设为2，则输入数据的大小变为(8,8)；如果将填充设 为3，则大小变为(10, 10)。 使用填充主要是为了调整输出的大小。比如，对大小为(4, 4)的输入数据应用(3,3)的滤波器时，输出大小变为(2, 2)，相当于输出大小比输入大小缩小了2个元素。在反复进行多次卷积运算的深度网络中,如果每次进行卷积运算都会缩小空间，那么在某个时刻输出大小就有可能变为1，导致无法再应用卷积运算。 2.2.3 步幅介绍完填充，再来介绍一下步幅。步幅是应用滤波器的位置间隔，以下是步幅为2的情况。 2.2.4 填充，步幅和输出结果的瓜系增大步幅后，输出大小会变小。而增大填充后，输出大小会变大。 如果将这样的关系写成算式，会如何呢？我们看一下对于填充和步幅，如何计算输出大小。 这里，假设输入大小为(H,W)，滤波器大小为(FH,FW)，输出大小为 (OH,OW)，填充为P，步幅为S。此时，输出大小可通过下列式子计算。 $$ OH = \frac{H+2P-FH}{S} + 1 $$ $$ OW = \frac{H+2P-FW}{S} + 1 $$ 我们来思考一个问题，为什么卷积运算是有效的？依然用例子来说明，比如我们有一个4*4的图像，我们设计两个卷积核，看看运用卷积核后图片会变成什么样。从结果可以看出，通过第一个卷积核计算后的特征图是一个三维数据，在第三列的绝对值最大，说明原始图片上对应的地方有一条垂直方向的特征，即像素数值变化较大；而通过第二个卷积核计算后，第三列的数值为0，第二行的数值绝对值最大，说明原始图片上对应的地方有一条水平方向的特征，即像素值数值变化较大。这样就提取出了大致的两个特征，通过在更深层次的网络中设置更多更巧妙的卷积核，我们就可以得到更多更准确的特征。我们设计的卷积核分别能够提取，或者说检测出原始图片的特定的特征。所以实际上就可以把卷积核理解为特征提取器，我们不需要手动去选取特征，只用设计好卷积核的尺寸，数量和滑动的步长就可以让它自己去训练了。同时，由于多个神经元可以共享卷积核，对于高位数据的处理将会变得非常简单。留下几个小问题供大家思考： 1.卷积核的尺寸必须为正方形吗？可以为长方形吗？如果是长方形应该怎么计算？ 2.卷积核的个数如何确定？每一层的卷积核的个数都是相同的吗？ 3.步长的向右和向下移动的幅度必须是一样的吗？ 在最后我将给出这些问题的答案。 2.2.5 多通道图的卷积运算这里先介绍单通道图和多通道图的概念（一）：单通道图俗称灰度图，每个像素点只能有有一个值表示颜色，它的像素值在0到255之间，0是黑色，255是白色，中间值是一些不同等级的灰色。（也有3通道的灰度图，3通道灰度图只有一个通道有值，其他两个通道的值都是零）。（二）:三通道图每个像素点都有3个值表示 ，所以就是3通道。也有4通道的图。例如RGB图片即为三通道图片，RGB色彩模式是工业界的一种颜色标准，是通过对红(R)、绿(G)、蓝(B)三个颜色通道的变化以及它们相互之间的叠加来得到各式各样的颜色的，RGB即是代表红、绿、蓝三个通道的颜色，这个标准几乎包括了人类视力所能感知的所有颜色，是目前运用最广的颜色系统之一。总之，每一个点由三个值表示。 之前的卷积运算的例子都是以有高、长方向的2维形状的单通道图为对象的。但是， 图像是3维数据，除了高、长方向之外，还需要处理通道方向。这里，我们按照与之前相同的顺序，看一下对加上了通道方向的3维数据进行卷积运算的例子。这里以3通道的数据为例， 展示了卷积运算的结果。和2维数据时相比，可以发现纵深方向（通道方向）上特征图增加了。通道方向上有多个特征图时，会按通道进行输入数据和滤波器的卷积运算，并将结果相加，从而得到输出。需要注意的是，在3维数据的卷积运算中，输入数据和滤波器的通道数要设为相同的值。从上图可知，每个卷积核输出一张特征图，而多个卷积核输出的特征图汇集在一起，传递给下一层，这就是CNN的处理流。最后再看看一看卷积运算中的处理流是什么亚子。 三、 池化讲了这么久的卷积，终于来到了池化（pooling）层池化是缩小高、长方向上的空间的运算。比如，如图所示，进行将 2×2的区域集约成1个元素的处理，缩小空间大小。一般来说，池化的窗口大小会 和步幅设定成相同的值。 除了MaxPooling之外，还有AveragePooling等。相对于MaxPooling是从目标区域中取出最大值，AveragePooling则是计算目标区域的平均值。 在图像识别领域，主要使用MaxPooling。 那么问题来了，为什么MaxPooling能起到效果呢MaxPooling意义在哪里？如果我们只取最大值，那其他的值被舍弃难道就没有影响吗？不会损失这部分信息吗？如果认为这些信息是可损失的，那么是否意味着我们在进行卷积操作后仍然产生了一些不必要的冗余信息呢？ 其实从上文分析卷积核为什么有效的原因来看，每一个卷积核可以看做一个特征提取器，不同的卷积核负责提取不同的特征，我们例子中设计的第一个卷积核能够提取出“垂直”方向的特征，第二个卷积核能够提取出“水平”方向的特征，那么我们对其进行MaxPooling操作后，提取出的是真正能够识别特征的数值，其余被舍弃的数值，对于我提取特定的特征并没有特别大的帮助。在进行后续计算时，就减小了特征图的尺寸，从而减少参数，达到减小计算量，缺不损失效果的情况。 这也意味着MaxPooling对微小的位置变化具有健壮性，输入数据发生微小偏差时，池化仍会返回相同的结果,池化会吸收输入数据的偏差（根据数据的不同，结果有可能不一致）。 不过并不是所有情况MaxPooling的效果都很好，有时候有些周边信息也会对某个特定特征的识别产生一定效果，那么这个时候舍弃这部分“不重要”的信息，就不划算了。所以还要具体情况具体分析，如果加了Max Pooling后效果反而变差了，不如把卷积后不加MaxPooling的结果与卷积后加了MaxPooling的结果输出对比一下，看看MaxPooling是否对卷积核提取特征起了反效果。 3.1 Flatten层 &amp; Fully Connected Layer 到这一步，其实我们的一个完整的“卷积部分”就算完成了，如果想要叠加层数，一般也是叠加“Conv-MaxPooing”,通过不断的设计卷积核的尺寸，数量，提取更多的特征，最后识别不同类别的物体。做完MaxPooling后，我们就会把这些数据“拍平”，丢到Flatten层，然后把Flatten层的output放到full connected Layer里，采用softmax对其进行分类。 四、 CNN的可视化学习前的滤波器是随机进行初始化的，所以在黑白的浓淡上没有规律可循，但学习后的滤波器变成了有规律的图像。我们发现，通过学习，滤波器被更新成了有规律的滤波器，比如从白到黑渐变的滤波器、含有块状区域（称为blob）的滤波器等。如果要问右边的有规律的滤波器在“观察”什么，答案就是它在观察边缘（颜色变化的分界线）和斑块（局部的块状区域）等。比如，左半部分为白色、右半部分为黑色的滤波器的情况下，如图所示，会对垂直方向上的边缘有响应。 五、问题小结最后是上面给大家留下的问题1.卷积核的尺寸必须为正方形吗？可以为长方形吗？如果是长方形应该怎么计算？ 2.卷积核的个数如何确定？每一层的卷积核的个数都是相同的吗？ 3.步长的向右和向下移动的幅度必须是一样的吗？ 下面的想法，可以作为参考： 1.卷积核的尺寸不一定非得为正方形。长方形也可以，只不过通常情况下为正方形。如果要设置为长方形，那么首先得保证这层的输出形状是整数，不能是小数（一些框架会对小数进行四舍五入处理，一些会引起报错）。如果你的图像是边长为 28 的正方形。那么卷积层的输出就满足 [ (28 - kernel_size)/ stride ] + 1 ，这个数值得是整数才行，否则没有物理意义。池化层同理。FC 层的输出形状总是满足整数，其唯一的要求就是整个训练过程中 FC 层的输入得是定长的。如果你的图像不是正方形。那么在制作数据时，可以缩放到统一大小（非正方形），再使用非正方形的卷积核来使得卷积层的输出依然是整数。总之，撇开网络结果设定的好坏不谈，其本质上就是在做算术应用题：如何使得各层的输出是整数。 2.由经验确定。通常情况下，靠近输入的卷积层，譬如第一层卷积层，会找出一些共性的特征，如手写数字识别中第一层我们设定卷积核个数为5个，一般是找出诸如”横线”、“竖线”、“斜线”等共性特征，我们称之为basic feature，经过池化后，在第二层卷积层，设定卷积核个数为20个，可以找出一些相对复杂的特征，如“横折”、“左半圆”、“右半圆”等特征，越往后，卷积核设定的数目越多，越能体现的特征就越细致，就越容易分类出来。比如你想分类出“0”的数字，你看到这个特征，能推测是什么数字呢？只有越往后，检测识别的特征越多，试过才能慢慢能识别出这几个特征，那么我就能够确定这个数字是“0”。 3.有stride_w和stride_h，表示左右步长和上下步长。如果用stride，则表示stride_h=stride_w=stride。]]></content>
      <categories>
        <category>neural network</category>
      </categories>
      <tags>
        <tag>CNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[神经网络的反向误差传播]]></title>
    <url>%2Fdesirelife6.github.io%2F2019%2F08%2F30%2F%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8F%8D%E5%90%91%E8%AF%AF%E5%B7%AE%E4%BC%A0%E6%92%AD%2F</url>
    <content type="text"><![CDATA[一、一般的全连接神经网络结构1.1 “层”的概念每个“层”在神经网络中被实现为一个类，作为神经网络中的功能单位，类中定义了此层接受的输入，运算和输出。比如，负责激活函数的层就是Sigmoid层，ReLU层等等，负责矩阵相乘的就是上次提到的Affine层。以Affine层和ReLU层为例 123456789101112131415161718AffineX = np.random.rand(2) # 输入W = np.random.rand(2，3) # 权重参数B = np.random.rand(3) # 偏置Y = np.dot(X, W) + B # 得到相乘结果class Relu: def __init__(self): self.mask = None # mask这个变量在反向传播时很重要，稍后会提到 def forward(self, x): # 正向传播 self.mask = (x &lt;= 0) out = x.copy() out[self.mask] = 0 return out def backward(self, dout): # 反向传播 dout[self.mask] = 0 dx = dout return dx 这里Affine中的 X、W、B 分别是形状为 (2,)、(2,3)、(3,) 的多维数组。这样一来，神经元的加权和可以用 Y = np.dot(X, W) + B 计算出来。然后，Y 经过激活函数转换后，传递给下一层。这就是神经网络正向传播的流程。调用激活层中的 forward 正向传播。 二、正向传播时的梯度下降法在正向传播中，采用数值微分法求梯度在每一层，采用数值微分法分别求损失函数对于该层的各个权重参数的偏导数，组合成梯度，然后令各参数向梯度下降的方向更新。代码其实很好懂，简直就像大白话，这里举一个简单的例子。 123456789101112131415161718192021222324# 求梯度def numerical_gradient(f, x): # x为传入的各参数的数组/多维数组 h = 1e-4 # 0.0001 grad = np.zeros_like(x) # 生成和x形状相同的数组 for idx in range(x.size): tmp_val = x[idx] # f(x+h)的计算 x[idx] = tmp_val + h fxh1 = f(x) # f(x-h)的计算 x[idx] = tmp_val - h fxh2 = f(x) # 组合为梯度 grad[idx] = (fxh1 - fxh2) / (2*h) x[idx] = tmp_val # 还原值 return grad# 梯度下降法def gradient_descent(f, init_x, lr=0.01, step_num=100): x = init_x for i in range(step_num): grad = numerical_gradient(f, x) x -= lr * grad return x 梯度下降法的函数的参数中，f 为我们选用的损失函数，x为各层传入的参数数组，lr为学习率，也就是参数每次更新的程度，step_num为更新的轮数。 这就是通过数值微分法计算损失函数关于权重参数的梯度。数值微分虽然简单，也容易实现，但缺点是计算上比较费时间。所以我们将学习一个能够高效计算权重参数的梯度的方法——误差反向传播法。 三、误差反向传播法3.1 计算图3.1.1 利用计算图求解计算图解题的情况下，需要按如下流程进行 构建计算图。 在计算图上，从左向右进行计算。（正向传播） 现在我们尝试用计算图求解简单的问题问题1： 太郎在超市买了2个100元一个的苹果，消费税是10%，请计算支付金额。 问题2：太郎在超市买了2个苹果、3个橘子。其中，苹果每个100元， 橘子每个150元。消费税是10%，请计算支付金额。 3.1.2 局部计算计算图的特征是可以通过传递“局部计算”获得最终结果。“局部”这个 词的意思是“与自己相关的某个小范围”。局部计算是指，无论全局发生了什么， 都能只根据与自己相关的信息输出接下来的结果。我们用一个具体的例子来说明局部计算。比如，在超市买了2个苹果和其他很多东西。此时，可以画出如图3-3所示的计算图。 如图3-3所示，假设（经过复杂的计算）购买的其他很多东西总共花费4000元。这里的重点是，各个节点处的计算都是局部计算。这意味着，例如苹果和其他很多东西的求和运算（4000 + 200 → 4200）并不关心4000这个数字是如何计算而来的，只要把两个数字相加就可以了。换言之，各个节点处只需进行与自己有关的计算（在这个例子中是对输入的两个数字进行加法运算），不用考虑全局。综上，计算图可以集中精力于局部计算。无论全局的计算有多么复杂， 各个步骤所要做的就是对象节点的局部计算。虽然局部计算非常简单，但是通过传递它的计算结果，可以获得全局的复杂计算的结果。 3.1.3 为何使用计算图实际上，使用计算图最大的原因是，可以通过反向传播高效计算导数。 在介绍计算图的反向传播时，我们再来思考一下问题1。问题1中，假设我们想知道苹果价格的上涨会在多大程度上影响最终的支付金额，即求“支付金额关于苹果的价格的导数”。设苹果的价格为 x，支付金额为 L，则相当于求 $ \frac{dL}{dX} $。先来看一下结果，如图3-4所示，可以通过计算图的反向传播求导数（关于如何进行反向传播，接下来马上会介绍）。如图3-4所示，反向传播使用与正方向相反的箭头（粗线）表示。反向传播传递“局部导数”，将导数的值写在箭头的下方。在这个例子中，反向传播从右向左传递导数的值（1→1.1→2.2）。从这个结果中可知，“支付金额关于苹果的价格的导数”的值是2.2 “支付金额关于消费税的导数”“支付金额关于苹果的个数的导数”等也都可以用同样的方式算出来。并且, 计算中途求得的导数的结果（中间传递的导数）可以被共享，从而可以高效地计算多个导数。综上，计算图的优点是，可以通过正向传播和反向传 播高效地计算各个变量的导数值。 3.2 链式法则3.2.1 计算图的反向传播话不多说，让我们先来看一个使用计算图的反向传播的例子。假设存在 y = f(x)的计算，这个计算的反向传播如图3-4所示。 如图所示，反向传播的计算顺序是，将信号E乘以节点的局部导数 （ $\frac{dy}{dx}$），然后将结果传递给下一个节点。这里所说的局部导数是指正向传播中$y = f(x) $的导数，也就是y关于x的导数（ $\frac{dy}{dx}$）。把这个局部导数乘以上游传过来的值（本例中为E）， 然后传递给前面的节点。 3.2.2 链式法则链式法则是关于复合函数的导数的性质，定义如下。 如果某个函数由复合函数表示，则该复合函数的导数可以用构成复合函数的各个函数的导数的乘积表示。 形如 $\frac{dZ}{dX} = \frac{dZ}{dt} \frac{dt}{dZ} $， 即为求导的链式法则。高数内容，不再赘述。 3.2.3 链式法则与计算图现在我们尝试将上式的链式法则的计算用计算图表示出来。如果用 “**2”节点表示平方运算的话，则计算图如图3-5所示。因为$\frac{dz}{dt} = 2t $; $\frac{dt}{dx} = 1 $ ; 所以$\frac{dz}{dx} = 2(x+y) $ 3.3 反向传播3.3.1 加法的反向传播加法反向传播将从上游传过来的导数（本例中是 ）乘以 1，然 后传向下游。也就是说，因为加法节点的反向传播只乘以1，所以输入的值 会原封不动地流向下一个节点。 3.3.2 乘法的反向传播乘法的反向传播会将上游的值乘以正向传播时的输入信号的“翻转值” 后传递给下游。翻转值表示一种翻转关系，如图5-12所示，正向传播时信号 是x的话，反向传播时则是y；正向传播时信号是y的话，反向传播时则是x。 练习结合加法和乘法，以苹果为例，尝试填入这些空格 3.4 激活函数层的反向传播实现3.4.1 ReLU 层激活函数ReLU的导数为如果正向传播时的输入x大于0，则反向传播会将上游的 值原封不动地传给下游。反过来，如果正向传播时的x小于等于0，则反向传播中传给下游的信号将停在此处所以在开始时提到的ReLU层的mask变量，是用来存储正向传播输入是否为0的变量，它是由True/False构成的NumPy数组，它会把正向传播时的输入x的元素中小于等于0的地方保存为True，其他地方（大于0的元素）保存为False。在反向传播中尤其重要。计算图表示即为 3.4.2 Sigmoid层步骤1“/”节点表示 ，它的导数可以解析性地表示为下式。$ \frac{dy}{dx} = - \frac{1}{x^2} = - y^2 $反向传播时，会将上游的值乘以$−y^2$（正向传播的输出的平方乘以−1后的值）后，再传给下游。 步骤2 “+”节点将上游的值原封不动地传给下游。 步骤3“exp”节点表示y = exp(x)，它的导数由下式表示。$ \frac{dy}{dx} = exp(x) $计算图中，上游的值乘以正向传播时的输出（这个例子中是exp(−x)）后， 再传给下游。 步骤4“×”节点将正向传播时的值翻转后做乘法运算。因此，这里要乘以−1。 最终结果为 这里要注意， 这个值只根据正向传播时的输入x和输出y就可以算出来,所以sigmoid的计算图可以简化为sigmoid节点，简洁版的计算图可以省略反向传播中的计算过程，因此计算效率更高。此外， 通过对节点进行集约化，可以不用在意Sigmoid层中琐碎的细节，而只需要专注它的输入和输出，这一点也很重要。 3.5 Affine/Softmax层的实现3.5.1 Affine层神经网络的正向传播中，为了计算加权信号的总和，使用了矩阵的乘积运算 12345AffineX = np.random.rand(2) # 输入W = np.random.rand(2，3) # 权重参数B = np.random.rand(3) # 偏置Y = np.dot(X, W) + B # 得到相乘结果 神经网络的正向传播中进行的矩阵的乘积运算在几何学领域被称为“仿 射变换” A。因此，这里将进行仿射变换的处理实现为“Affine层”。 现在将它用计算图表示出来，要注意X、W、B是矩阵（多维数组）。 之前我们见到的计算图中各个节点间流动的是标量，而这个例子中各个节点间传播的是矩阵。现在我们来考虑图 3-14 的计算图的反向传播。以矩阵为对象的反向传播，通过数学推导可得到下式$$ \frac{dL}{dX} = \frac{dL}{dY} * W^T $$$$ \frac{dL}{dW} = X^T * \frac{dL}{dY} $$表示为计算图即为：其中值得注意的是,X与$\frac{dL}{dX}$形状相同，W与$\frac{dL}{dW}$形状相同,从下式也容易看出 3.5.2 批版本的Affine层前面介绍的Affi ne层的输入X是以单个数据为对象的。现在我们考虑N 个数据一起进行正向传播的情况，也就是批版本的Affine层。先来看计算图与刚刚不同的是，现在输入X的形状是(N,2)。之后就和前面一样，在计算图上进行单纯的矩阵计算。加上偏置时，需要特别注意。正向传播时，偏置被加到X·W的各个数据上。比如，N = 2（数据为2个）时，偏置会被分别加到这2个数据（各自 的计算结果）上。因此， 反向传播时，各个数据的反向传播的值需要汇总为偏置的元素。 12345678&gt;&gt;&gt; dY = np.array([[1, 2, 3,], [4, 5, 6]]) &gt;&gt;&gt; dY array([[1, 2, 3], [4, 5, 6]]) &gt;&gt;&gt; &gt;&gt;&gt; dB = np.sum(dY, axis=0) &gt;&gt;&gt; dB array([5, 7, 9]) 这个例子中，假定数据有2个（N = 2）。偏置的反向传播会对这2个数据的导数按元素进行求和。因此，这里使用了np.sum()对第0轴（以数据为单位的轴，axis=0）方向上的元素进行求和。 3.5.3 Softmax-with-Loss 层上次我们提到过，softmax函数会将输入值正规化之后再输出。比如手写数字识别时，Softmax层的输出如下所示。 神经网络中进行的处理有推理（inference）和学习两个阶段。神经网络的推理通常不使用Softmax层。比如，用上图的网络进行推理时， 会将最后一个Affine层的输出作为识别结果。神经网络中未被正规化的输出结果（Softmax层前面的Affine层的输出）有时被称为“得分”。也就是说，当神经网络的推理只需要给出一个答案的情况下，因为此时只对得分最大值感兴趣，所以不需要Softmax层。 不过，神经网络的学习阶段则需要Softmax层。 来看softmax的计算图，导出过程比较复杂，这里只给出最终结果 不感兴趣的话可以看简化版本 Softmax层的反向传播得到了 $（y_1 −t_1,y_2 −t_2,y_3 −t_3）$这样“漂亮”的结果。由于$（y_1,y_2,y_3）$是 Softmax层的输出，$（ t_1,t_2,t_3）$是监督数据，所以$（y_1 −t_1,y_2 −t_2,y_3 −t_3）$是 Softmax层的输出和教师标签的差分。神经网络的反向传播会把这个差分表示的误差传递给前面的层，这是神经网络学习中的重要性质。神经网络学习的目的就是通过调整权重参数，使神经网络的输出（Softmax 的输出）接近教师标签。因此，必须将神经网络的输出与教师标签的误差高效地传递给前面的层。刚刚的$（y_1 −t_1,y_2 −t_2,y_3 −t_3）$正是 Softmax层的输出与教师标签的差，直截了当地表示了当前神经网络的输出与教师标签的误差。 使用交叉熵误差作为softmax函数的损失函数后，反向传播得到 $（y_1 −t_1,y_2 −t_2,y_3 −t_3）$这样 “漂亮”的结果。实际上，这样“漂亮” 的结果并不是偶然的，而是为了得到这样的结果，特意设计了交叉熵误差函数。回归问题中输出层使用“恒等函数”，损失函数使用 “平方和误差”，也是出于同样的理由。也就是说，使用“平 方和误差”作为“恒等函数”的损失函数，反向传播才能得到$（y_1 −t_1,y_2 −t_2,y_3 −t_3）$这样“漂亮”的结果。 四、神经网络学习全貌前提神经网络中有合适的权重和偏置，调整权重和偏置以便拟合训练数据的过程称为学习神经网络的学习分为下面4个步骤。步骤1（mini-batch）从训练数据中随机选择一部分数据。步骤2（计算梯度）计算损失函数关于各个权重参数的梯度。步骤3（更新参数）将权重参数沿梯度方向进行微小的更新。步骤4（重复）重复步骤1、步骤2、步骤3。这次介绍的误差反向传播法会在步骤2中出现。上一次中，我们利用数值微分求得了这个梯度。数值微分虽然实现简单，但是计算要耗费较多的时间，而误差反向传播法可以快速高效地计算梯度。 还有一篇CNN噢，欢迎持续关注 :-D]]></content>
      <categories>
        <category>neural network</category>
      </categories>
      <tags>
        <tag>neural network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2Fdesirelife6.github.io%2F2019%2F08%2F30%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
  <entry>
    <title><![CDATA[关于作者]]></title>
    <url>%2Fdesirelife6.github.io%2Fabout%2Findex.html</url>
    <content type="text"><![CDATA[我是谁我是一枚爱吃饭爱睡觉的可爱程序猿博客干嘛用我想干嘛干嘛鸭嘻嘻嘻其实可以来github给我star ： https://github.com/Desirelife6]]></content>
  </entry>
  <entry>
    <title><![CDATA[categories]]></title>
    <url>%2Fdesirelife6.github.io%2Fcategories%2Findex.html</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[tags]]></title>
    <url>%2Fdesirelife6.github.io%2Ftags%2Findex.html</url>
    <content type="text"></content>
  </entry>
</search>
