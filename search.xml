<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[神经网络的反向误差传播]]></title>
    <url>%2Fdesirelife6.github.io%2F2019%2F08%2F30%2F%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8F%8D%E5%90%91%E8%AF%AF%E5%B7%AE%E4%BC%A0%E6%92%AD%2F</url>
    <content type="text"><![CDATA[一、一般的全连接神经网络结构1.1 “层”的概念每个“层”在神经网络中被实现为一个类，作为神经网络中的功能单位，类中定义了此层接受的输入，运算和输出。比如，负责激活函数的层就是Sigmoid层，ReLU层等等，负责矩阵相乘的就是上次提到的Affine层。以Affine层和ReLU层为例 123456789101112131415161718AffineX = np.random.rand(2) # 输入W = np.random.rand(2，3) # 权重参数B = np.random.rand(3) # 偏置Y = np.dot(X, W) + B # 得到相乘结果class Relu: def __init__(self): self.mask = None # mask这个变量在反向传播时很重要，稍后会提到 def forward(self, x): # 正向传播 self.mask = (x &lt;= 0) out = x.copy() out[self.mask] = 0 return out def backward(self, dout): # 反向传播 dout[self.mask] = 0 dx = dout return dx 这里Affine中的 X、W、B 分别是形状为 (2,)、(2,3)、(3,) 的多维数组。这样一来，神经元的加权和可以用 Y = np.dot(X, W) + B 计算出来。然后，Y 经过激活函数转换后，传递给下一层。这就是神经网络正向传播的流程。调用激活层中的 forward 正向传播。 二、正向传播时的梯度下降法在正向传播中，采用数值微分法求梯度在每一层，采用数值微分法分别求损失函数对于该层的各个权重参数的偏导数，组合成梯度，然后令各参数向梯度下降的方向更新。代码其实很好懂，简直就像大白话，这里举一个简单的例子。 123456789101112131415161718192021222324# 求梯度def numerical_gradient(f, x): # x为传入的各参数的数组/多维数组 h = 1e-4 # 0.0001 grad = np.zeros_like(x) # 生成和x形状相同的数组 for idx in range(x.size): tmp_val = x[idx] # f(x+h)的计算 x[idx] = tmp_val + h fxh1 = f(x) # f(x-h)的计算 x[idx] = tmp_val - h fxh2 = f(x) # 组合为梯度 grad[idx] = (fxh1 - fxh2) / (2*h) x[idx] = tmp_val # 还原值 return grad# 梯度下降法def gradient_descent(f, init_x, lr=0.01, step_num=100): x = init_x for i in range(step_num): grad = numerical_gradient(f, x) x -= lr * grad return x 梯度下降法的函数的参数中，f 为我们选用的损失函数，x为各层传入的参数数组，lr为学习率，也就是参数每次更新的程度，step_num为更新的轮数。 这就是通过数值微分法计算损失函数关于权重参数的梯度。数值微分虽然简单，也容易实现，但缺点是计算上比较费时间。所以我们将学习一个能够高效计算权重参数的梯度的方法——误差反向传播法。 三、误差反向传播法3.1 计算图3.1.1 利用计算图求解计算图解题的情况下，需要按如下流程进行 构建计算图。 在计算图上，从左向右进行计算。（正向传播） 现在我们尝试用计算图求解简单的问题问题1： 太郎在超市买了2个100元一个的苹果，消费税是10%，请计算支付金额。 问题2：太郎在超市买了2个苹果、3个橘子。其中，苹果每个100元， 橘子每个150元。消费税是10%，请计算支付金额。 3.1.2 局部计算计算图的特征是可以通过传递“局部计算”获得最终结果。“局部”这个 词的意思是“与自己相关的某个小范围”。局部计算是指，无论全局发生了什么， 都能只根据与自己相关的信息输出接下来的结果。我们用一个具体的例子来说明局部计算。比如，在超市买了2个苹果和其他很多东西。此时，可以画出如图3-3所示的计算图。 如图3-3所示，假设（经过复杂的计算）购买的其他很多东西总共花费4000元。这里的重点是，各个节点处的计算都是局部计算。这意味着，例如苹果和其他很多东西的求和运算（4000 + 200 → 4200）并不关心4000这个数字是如何计算而来的，只要把两个数字相加就可以了。换言之，各个节点处只需进行与自己有关的计算（在这个例子中是对输入的两个数字进行加法运算），不用考虑全局。综上，计算图可以集中精力于局部计算。无论全局的计算有多么复杂， 各个步骤所要做的就是对象节点的局部计算。虽然局部计算非常简单，但是通过传递它的计算结果，可以获得全局的复杂计算的结果。 3.1.3 为何使用计算图实际上，使用计算图最大的原因是，可以通过反向传播高效计算导数。 在介绍计算图的反向传播时，我们再来思考一下问题1。问题1中，假设我们想知道苹果价格的上涨会在多大程度上影响最终的支付金额，即求“支付金额关于苹果的价格的导数”。设苹果的价格为 x，支付金额为 L，则相当于求 $ \frac{dL}{dX} $。先来看一下结果，如图3-4所示，可以通过计算图的反向传播求导数（关于如何进行反向传播，接下来马上会介绍）。如图3-4所示，反向传播使用与正方向相反的箭头（粗线）表示。反向传播传递“局部导数”，将导数的值写在箭头的下方。在这个例子中，反向传播从右向左传递导数的值（1→1.1→2.2）。从这个结果中可知，“支付金额关于苹果的价格的导数”的值是2.2 “支付金额关于消费税的导数”“支付金额关于苹果的个数的导数”等也都可以用同样的方式算出来。并且, 计算中途求得的导数的结果（中间传递的导数）可以被共享，从而可以高效地计算多个导数。综上，计算图的优点是，可以通过正向传播和反向传 播高效地计算各个变量的导数值。 3.2 链式法则3.2.1 计算图的反向传播话不多说，让我们先来看一个使用计算图的反向传播的例子。假设存在 y = f(x)的计算，这个计算的反向传播如图3-4所示。 如图所示，反向传播的计算顺序是，将信号E乘以节点的局部导数 （ $\frac{dy}{dx}$），然后将结果传递给下一个节点。这里所说的局部导数是指正向传播中$y = f(x) $的导数，也就是y关于x的导数（ $\frac{dy}{dx}$）。把这个局部导数乘以上游传过来的值（本例中为E）， 然后传递给前面的节点。 3.2.2 链式法则链式法则是关于复合函数的导数的性质，定义如下。 如果某个函数由复合函数表示，则该复合函数的导数可以用构成复合函数的各个函数的导数的乘积表示。 形如 $\frac{dZ}{dX} = \frac{dZ}{dt} \frac{dt}{dZ} $， 即为求导的链式法则。高数内容，不再赘述。 3.2.3 链式法则与计算图现在我们尝试将上式的链式法则的计算用计算图表示出来。如果用 “**2”节点表示平方运算的话，则计算图如图3-5所示。因为$\frac{dz}{dt} = 2t $; $\frac{dt}{dx} = 1 $ ; 所以$\frac{dz}{dx} = 2(x+y) $ 3.3 反向传播3.3.1 加法的反向传播加法反向传播将从上游传过来的导数（本例中是 ）乘以 1，然 后传向下游。也就是说，因为加法节点的反向传播只乘以1，所以输入的值 会原封不动地流向下一个节点。 3.3.2 乘法的反向传播乘法的反向传播会将上游的值乘以正向传播时的输入信号的“翻转值” 后传递给下游。翻转值表示一种翻转关系，如图5-12所示，正向传播时信号 是x的话，反向传播时则是y；正向传播时信号是y的话，反向传播时则是x。 练习结合加法和乘法，以苹果为例，尝试填入这些空格 3.4 激活函数层的反向传播实现3.4.1 ReLU 层激活函数ReLU的导数为如果正向传播时的输入x大于0，则反向传播会将上游的 值原封不动地传给下游。反过来，如果正向传播时的x小于等于0，则反向传播中传给下游的信号将停在此处所以在开始时提到的ReLU层的mask变量，是用来存储正向传播输入是否为0的变量，它是由True/False构成的NumPy数组，它会把正向传播时的输入x的元素中小于等于0的地方保存为True，其他地方（大于0的元素）保存为False。在反向传播中尤其重要。计算图表示即为 3.4.2 Sigmoid层步骤1“/”节点表示 ，它的导数可以解析性地表示为下式。$ \frac{dy}{dx} = - \frac{1}{x^2} = - y^2 $反向传播时，会将上游的值乘以$−y^2$（正向传播的输出的平方乘以−1后的值）后，再传给下游。 步骤2 “+”节点将上游的值原封不动地传给下游。 步骤3“exp”节点表示y = exp(x)，它的导数由下式表示。$ \frac{dy}{dx} = exp(x) $计算图中，上游的值乘以正向传播时的输出（这个例子中是exp(−x)）后， 再传给下游。 步骤4“×”节点将正向传播时的值翻转后做乘法运算。因此，这里要乘以−1。 最终结果为 这里要注意， 这个值只根据正向传播时的输入x和输出y就可以算出来,所以sigmoid的计算图可以简化为sigmoid节点，简洁版的计算图可以省略反向传播中的计算过程，因此计算效率更高。此外， 通过对节点进行集约化，可以不用在意Sigmoid层中琐碎的细节，而只需要专注它的输入和输出，这一点也很重要。 3.5 Affine/Softmax层的实现3.5.1 Affine层神经网络的正向传播中，为了计算加权信号的总和，使用了矩阵的乘积运算 12345AffineX = np.random.rand(2) # 输入W = np.random.rand(2，3) # 权重参数B = np.random.rand(3) # 偏置Y = np.dot(X, W) + B # 得到相乘结果 神经网络的正向传播中进行的矩阵的乘积运算在几何学领域被称为“仿 射变换” A。因此，这里将进行仿射变换的处理实现为“Affine层”。 现在将它用计算图表示出来，要注意X、W、B是矩阵（多维数组）。 之前我们见到的计算图中各个节点间流动的是标量，而这个例子中各个节点间传播的是矩阵。现在我们来考虑图 3-14 的计算图的反向传播。以矩阵为对象的反向传播，通过数学推导可得到下式$$ \frac{dL}{dX} = \frac{dL}{dY} * W^T $$$$ \frac{dL}{dW} = X^T * \frac{dL}{dY} $$表示为计算图即为：其中值得注意的是,X与$\frac{dL}{dX}$形状相同，W与$\frac{dL}{dW}$形状相同,从下式也容易看出 3.5.2 批版本的Affine层前面介绍的Affi ne层的输入X是以单个数据为对象的。现在我们考虑N 个数据一起进行正向传播的情况，也就是批版本的Affine层。先来看计算图与刚刚不同的是，现在输入X的形状是(N,2)。之后就和前面一样，在计算图上进行单纯的矩阵计算。加上偏置时，需要特别注意。正向传播时，偏置被加到X·W的各个数据上。比如，N = 2（数据为2个）时，偏置会被分别加到这2个数据（各自 的计算结果）上。因此， 反向传播时，各个数据的反向传播的值需要汇总为偏置的元素。 12345678&gt;&gt;&gt; dY = np.array([[1, 2, 3,], [4, 5, 6]]) &gt;&gt;&gt; dY array([[1, 2, 3], [4, 5, 6]]) &gt;&gt;&gt; &gt;&gt;&gt; dB = np.sum(dY, axis=0) &gt;&gt;&gt; dB array([5, 7, 9]) 这个例子中，假定数据有2个（N = 2）。偏置的反向传播会对这2个数据的导数按元素进行求和。因此，这里使用了np.sum()对第0轴（以数据为单位的轴，axis=0）方向上的元素进行求和。 3.5.3 Softmax-with-Loss 层上次我们提到过，softmax函数会将输入值正规化之后再输出。比如手写数字识别时，Softmax层的输出如下所示。 神经网络中进行的处理有推理（inference）和学习两个阶段。神经网络的推理通常不使用Softmax层。比如，用上图的网络进行推理时， 会将最后一个Affine层的输出作为识别结果。神经网络中未被正规化的输出结果（Softmax层前面的Affine层的输出）有时被称为“得分”。也就是说，当神经网络的推理只需要给出一个答案的情况下，因为此时只对得分最大值感兴趣，所以不需要Softmax层。 不过，神经网络的学习阶段则需要Softmax层。 来看softmax的计算图，导出过程比较复杂，这里只给出最终结果 不感兴趣的话可以看简化版本 Softmax层的反向传播得到了 $（y_1 −t_1,y_2 −t_2,y_3 −t_3）$这样“漂亮”的结果。由于$（y_1,y_2,y_3）$是 Softmax层的输出，$（ t_1,t_2,t_3）$是监督数据，所以$（y_1 −t_1,y_2 −t_2,y_3 −t_3）$是 Softmax层的输出和教师标签的差分。神经网络的反向传播会把这个差分表示的误差传递给前面的层，这是神经网络学习中的重要性质。神经网络学习的目的就是通过调整权重参数，使神经网络的输出（Softmax 的输出）接近教师标签。因此，必须将神经网络的输出与教师标签的误差高效地传递给前面的层。刚刚的$（y_1 −t_1,y_2 −t_2,y_3 −t_3）$正是 Softmax层的输出与教师标签的差，直截了当地表示了当前神经网络的输出与教师标签的误差。 使用交叉熵误差作为softmax函数的损失函数后，反向传播得到 $（y_1 −t_1,y_2 −t_2,y_3 −t_3）$这样 “漂亮”的结果。实际上，这样“漂亮” 的结果并不是偶然的，而是为了得到这样的结果，特意设计了交叉熵误差函数。回归问题中输出层使用“恒等函数”，损失函数使用 “平方和误差”，也是出于同样的理由。也就是说，使用“平 方和误差”作为“恒等函数”的损失函数，反向传播才能得到$（y_1 −t_1,y_2 −t_2,y_3 −t_3）$这样“漂亮”的结果。 四、神经网络学习全貌前提神经网络中有合适的权重和偏置，调整权重和偏置以便拟合训练数据的过程称为学习神经网络的学习分为下面4个步骤。步骤1（mini-batch）从训练数据中随机选择一部分数据。步骤2（计算梯度）计算损失函数关于各个权重参数的梯度。步骤3（更新参数）将权重参数沿梯度方向进行微小的更新。步骤4（重复）重复步骤1、步骤2、步骤3。这次介绍的误差反向传播法会在步骤2中出现。上一次中，我们利用数值微分求得了这个梯度。数值微分虽然实现简单，但是计算要耗费较多的时间，而误差反向传播法可以快速高效地计算梯度。 还有一篇CNN噢，欢迎持续关注 :-D]]></content>
      <categories>
        <category>neural network</category>
      </categories>
      <tags>
        <tag>neural network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2Fdesirelife6.github.io%2F2019%2F08%2F30%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
  <entry>
    <title><![CDATA[关于作者]]></title>
    <url>%2Fdesirelife6.github.io%2Fabout%2Findex.html</url>
    <content type="text"><![CDATA[我是谁我是一枚爱吃饭爱睡觉的可爱程序猿博客干嘛用我想干嘛干嘛鸭嘻嘻嘻其实可以来github给我star ： https://github.com/Desirelife6]]></content>
  </entry>
  <entry>
    <title><![CDATA[tags]]></title>
    <url>%2Fdesirelife6.github.io%2Ftags%2Findex.html</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[categories]]></title>
    <url>%2Fdesirelife6.github.io%2Fcategories%2Findex.html</url>
    <content type="text"></content>
  </entry>
</search>
